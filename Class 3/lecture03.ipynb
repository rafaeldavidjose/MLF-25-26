{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "304b0c06",
   "metadata": {},
   "source": [
    "# Machine Learning Fundamentals - Lecture 03\n",
    "\n",
    "This is the Jupyter notebook for Lecture 03 of the Machine Learning Fundamentals\n",
    "course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "99f40c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries using the commonly use short names (pd, sns, ...)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# The Path object from pathlib allows us to easily build paths in an\n",
    "# OS-independent fashion\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the required scikit-learn classes and functions\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Set a nicer style for Seaborn plots\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b4f293",
   "metadata": {},
   "source": [
    "## Part 1: load and clean the Pok√©mon dataset\n",
    "\n",
    "Here we just repeat the steps already done in the previous lectures, but in a\n",
    "more succint way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "761fb22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (note the use of the Path object)\n",
    "df = pd.read_csv(Path(\"Pokemon.csv\"))\n",
    "\n",
    "# It's not good practice to have column names with spaces and other non-standard\n",
    "# characters, so let's fix this by renaming the columns to standard names\n",
    "df.rename(columns={\n",
    "    \"Type 1\" : \"Type1\",\n",
    "    \"Type 2\" : \"Type2\",\n",
    "    \"Sp. Atk\" : \"SpAtk\",\n",
    "    \"Sp. Def\" : \"SpDef\",\n",
    "}, inplace=True)\n",
    "\n",
    "# Replace missing values in the \"Type2\" column with the string \"None\"\n",
    "df[\"Type2\"] = df[\"Type2\"].fillna(\"None\")\n",
    "\n",
    "# Since primary and secondary types are essentially categories (and not just\n",
    "# strings / objects), we can convert these columns to the category type\n",
    "df[\"Type1\"] = df[\"Type1\"].astype(\"category\")\n",
    "df[\"Type2\"] = df[\"Type2\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07466c93",
   "metadata": {},
   "source": [
    "Before we proceed to the interesting part, we'll perform our data scaling and\n",
    "train/test data splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "41193ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use all features except the Total, which can be considered redundant\n",
    "# since it's the total of the other features\n",
    "features = [\"HP\", \"Attack\", \"Defense\", \"SpAtk\", \"SpDef\", \"Speed\"]\n",
    "\n",
    "# Get only the specified features\n",
    "df_X = df[features]\n",
    "\n",
    "# Standardize them\n",
    "ss = StandardScaler()\n",
    "X = ss.fit_transform(df_X)\n",
    "\n",
    "# Our labels will be the legendary status\n",
    "y_leg = df[\"Legendary\"].to_numpy()\n",
    "\n",
    "# Let's split our data into training (80%) and test (20%) sets\n",
    "# Change the random_state parameter do split data in different ways\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_leg, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648147a5",
   "metadata": {},
   "source": [
    "## Part 2: Implement our own $k$-Nearest Neighbors classifier and regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "cb6ed975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this variable to change k for all the tests in this section\n",
    "k_for_all = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "32f93608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classify(X_Train, y_train, X_test, k=5):\n",
    "    dists = euclidean_distances(X_test, X_Train)\n",
    "    \n",
    "    idx_k_min = np.argpartition(dists, k, axis=1)[:, :k]\n",
    "\n",
    "    labels_k_min = y_train[idx_k_min]\n",
    "    \n",
    "    num_pred = X_test.shape[0]\n",
    "    \n",
    "    maj_labels = np.zeros(num_pred, dtype=y_train.dtype)\n",
    "\n",
    "    for i, row in enumerate(labels_k_min):\n",
    "        \n",
    "       values, counts = np.unique(row, return_counts=True)\n",
    "       \n",
    "       i_max = np.argmax(counts)\n",
    "       \n",
    "       maj_labels[i] = values[i_max]\n",
    "       \n",
    "    return maj_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "8c584aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (our kNN): 0.9250\n",
      "Accuracy (sklearn kNN): 0.9250\n"
     ]
    }
   ],
   "source": [
    "y_pred_ours = knn_classify(X_train, y_train, X_test, k=k_for_all)\n",
    "accuracy_ours = accuracy_score(y_test, y_pred_ours)\n",
    "\n",
    "knnClf = KNeighborsClassifier(n_neighbors=k_for_all)\n",
    "knnClf.fit(X_train, y_train)\n",
    "y_pred_knn = knnClf.predict(X_test)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "print(f\"Accuracy (our kNN): {accuracy_ours:.4f}\")\n",
    "print(f\"Accuracy (sklearn kNN): {accuracy_knn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "3d4c4ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_ours, y_pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "04552fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_regression(X_Train, y_train, X_test, k=5):\n",
    "    dists = euclidean_distances(X_test, X_Train)\n",
    "    \n",
    "    idx_k_min = np.argpartition(dists, k, axis=1)[:, :k]\n",
    "\n",
    "    return y_train[idx_k_min].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "b2a59891",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_total = df[\"Total\"].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_total, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "932cf851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.832500000000005"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_regr_ours = knn_regression(X_train, y_train, X_test, k=k_for_all)\n",
    "mean_absolute_error(y_test, y_regr_ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "19018474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031841346483808264"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_percentage_error(y_test, y_regr_ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "e3319ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.832500000000005"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnRegr = KNeighborsRegressor()\n",
    "knnRegr.fit(X_train, y_train)\n",
    "y_regr_knn = knnRegr.predict(X_test)\n",
    "mean_absolute_error(y_test, y_regr_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "add53c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031841346483808264"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_percentage_error(y_test, y_regr_ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "8b17cd37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_percentage_error(y_regr_ours, y_regr_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ae20f",
   "metadata": {},
   "source": [
    "# Mini-project 3 - Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576a1b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Entropy and information gain\n",
    "# -------------------------------------------\n",
    "\n",
    "def calculate_entropy(y):\n",
    "    \"\"\"\n",
    "    Calculates entropy for a 1D array.\n",
    "    \"\"\"\n",
    "    vals, counts = np.unique(y, return_counts=True) # unique classes and counts\n",
    "    probs = counts / counts.sum() # class probabilities\n",
    "    \n",
    "    return stats.entropy(probs)\n",
    "\n",
    "def best_threshold_for_feature(X, y, feature):\n",
    "    \"\"\"\n",
    "    Finds the best threshold for one feature.\n",
    "    Midpoints between unique sorted values.\n",
    "    Left branch '<=' and right '>'.\n",
    "    Returns (best_threshold, best_gain).\n",
    "    \"\"\"\n",
    "    x = X[feature].to_numpy()\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    uniq = np.unique(x) # unique sorted values\n",
    "    if uniq.size <= 1: # cannot split\n",
    "        return None, 0.0\n",
    "\n",
    "    midpoints = (uniq[:-1] + uniq[1:]) / 2.0\n",
    "    base_e = calculate_entropy(y) # base entropy\n",
    "    n = y.size\n",
    "\n",
    "    best_thr = None\n",
    "    best_gain = 0.0\n",
    "\n",
    "    # loop over thresholds\n",
    "    for thr in midpoints:\n",
    "        left_mask = x <= thr\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        yl = y[left_mask]\n",
    "        yr = y[right_mask]\n",
    "        if yl.size == 0 or yr.size == 0:\n",
    "            continue\n",
    "\n",
    "        el = calculate_entropy(yl) # left entropy\n",
    "        er = calculate_entropy(yr) # right entropy\n",
    "        weighted = (yl.size / n) * el + (yr.size / n) * er\n",
    "        gain = base_e - weighted\n",
    "\n",
    "        if gain > best_gain: # keep best split\n",
    "            best_gain = gain\n",
    "            best_thr = thr\n",
    "\n",
    "    return best_thr, best_gain\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# Tree building and prediction\n",
    "# -------------------------------------------\n",
    "\n",
    "def majority_class(y):\n",
    "    \"\"\"\n",
    "    Returns the most common label (majority vote).\n",
    "    \"\"\"\n",
    "    vals, counts = np.unique(y, return_counts=True)\n",
    "    return vals[np.argmax(counts)]\n",
    "\n",
    "def build_tree(X, y, features, max_depth=None, depth=0):\n",
    "    \"\"\"\n",
    "    Recursively builds a decision tree with numeric splits.\n",
    "    Stops when:\n",
    "    - node is pure (only one class)\n",
    "    - max_depth reached\n",
    "    \"\"\"\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    # pure node\n",
    "    if np.unique(y).size == 1:\n",
    "        return {\"leaf\": True, \"class\": y[0]}\n",
    "\n",
    "    # stop rules\n",
    "    if (max_depth is not None and depth >= max_depth):\n",
    "        return {\"leaf\": True, \"class\": majority_class(y)}\n",
    "\n",
    "    # single pass over features to find the best split\n",
    "    best_feat = None\n",
    "    best_thr = None\n",
    "    best_gain = -1.0 # start below any real gain\n",
    "\n",
    "    for feature in features:\n",
    "        thr, gain = best_threshold_for_feature(X, y, feature)\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_feat = feature\n",
    "            best_thr = thr\n",
    "\n",
    "    # no useful split\n",
    "    if best_thr is None or best_gain <= 0.0:\n",
    "        return {\"leaf\": True, \"class\": majority_class(y)}\n",
    "\n",
    "    # split data\n",
    "    left_mask = X[best_feat].to_numpy() <= best_thr\n",
    "    right_mask = ~left_mask\n",
    "    if not left_mask.any() or not right_mask.any():\n",
    "        return {\"leaf\": True, \"class\": majority_class(y)}\n",
    "\n",
    "    # grow children\n",
    "    left_child = build_tree(X[left_mask], y[left_mask], features, max_depth, depth + 1)\n",
    "    right_child = build_tree(X[right_mask], y[right_mask], features, max_depth, depth + 1)\n",
    "\n",
    "    # internal node\n",
    "    return {\n",
    "        \"leaf\": False,\n",
    "        \"feature\": best_feat,\n",
    "        \"threshold\": float(best_thr),\n",
    "        \"left\": left_child,\n",
    "        \"right\": right_child\n",
    "    }\n",
    "\n",
    "def predict_one(tree, row):\n",
    "    \"\"\"\n",
    "    Predicts one sample (row is a Series).\n",
    "    Walks down the tree until a leaf.\n",
    "    \"\"\"\n",
    "    node = tree\n",
    "    while not node[\"leaf\"]:\n",
    "        feature = node[\"feature\"]\n",
    "        thr = node[\"threshold\"]\n",
    "        node = node[\"left\"] if row[feature] <= thr else node[\"right\"]\n",
    "    return node[\"class\"]\n",
    "\n",
    "def predict_tree(tree, X):\n",
    "    \"\"\"\n",
    "    Predicts all rows in X.\n",
    "    Uses a list comprehension.\n",
    "    \"\"\"\n",
    "    return np.array([predict_one(tree, X.iloc[i]) for i in range(X.shape[0])])\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# Fit and classify\n",
    "# -------------------------------------------\n",
    "\n",
    "def fit_tree(X_train, y_train, features, max_depth=None):\n",
    "    \"\"\"\n",
    "    Trains the tree (just calls build_tree) and returns it.\n",
    "    \"\"\"\n",
    "    return build_tree(X_train, y_train, features, max_depth=max_depth)\n",
    "\n",
    "def dt_classify(X_train, y_train, X_test, features, max_depth=None):\n",
    "    \"\"\"\n",
    "    Trains on (X_train, y_train) and returns predictions for X_test.\n",
    "    \"\"\"\n",
    "    tree = fit_tree(X_train, y_train, features, max_depth=max_depth)\n",
    "    return predict_tree(tree, X_test)\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# Text tree printer\n",
    "# -------------------------------------------\n",
    "\n",
    "def print_tree(tree, indent=\"\", decimals=2):\n",
    "    \"\"\"\n",
    "    Prints the tree in text. Left is '<=', right is '>'.\n",
    "    \"\"\"\n",
    "    if tree[\"leaf\"]:\n",
    "        print(indent + \"-> class:\", tree[\"class\"])\n",
    "        return\n",
    "    feature = tree[\"feature\"]; thr = tree[\"threshold\"]\n",
    "    print(indent + f\"[{feature} <= {thr:.{decimals}f}]\")\n",
    "    print_tree(tree[\"left\"], indent + \"  \", decimals=decimals)\n",
    "    print(indent + f\"[{feature} >  {thr:.{decimals}f}]\")\n",
    "    print_tree(tree[\"right\"], indent + \"  \", decimals=decimals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "99624a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Decision Tree:\n",
      "[HP <= 78.50]\n",
      "  [Speed <= 114.50]\n",
      "    [SpDef <= 157.00]\n",
      "      -> class: False\n",
      "    [SpDef >  157.00]\n",
      "      -> class: False\n",
      "  [Speed >  114.50]\n",
      "    [SpAtk <= 87.50]\n",
      "      -> class: False\n",
      "    [SpAtk >  87.50]\n",
      "      -> class: False\n",
      "[HP >  78.50]\n",
      "  [Speed <= 83.00]\n",
      "    [HP <= 81.00]\n",
      "      -> class: False\n",
      "    [HP >  81.00]\n",
      "      -> class: False\n",
      "  [Speed >  83.00]\n",
      "    [SpDef <= 71.00]\n",
      "      -> class: False\n",
      "    [SpDef >  71.00]\n",
      "      -> class: True\n",
      "\n",
      "Sklearn Decision Tree:\n",
      "|--- HP <= 78.50\n",
      "|   |--- Speed <= 114.50\n",
      "|   |   |--- SpDef <= 157.00\n",
      "|   |   |   |--- class: False\n",
      "|   |   |--- SpDef >  157.00\n",
      "|   |   |   |--- class: False\n",
      "|   |--- Speed >  114.50\n",
      "|   |   |--- SpAtk <= 87.50\n",
      "|   |   |   |--- class: False\n",
      "|   |   |--- SpAtk >  87.50\n",
      "|   |   |   |--- class: False\n",
      "|--- HP >  78.50\n",
      "|   |--- Speed <= 83.00\n",
      "|   |   |--- HP <= 81.00\n",
      "|   |   |   |--- class: False\n",
      "|   |   |--- HP >  81.00\n",
      "|   |   |   |--- class: False\n",
      "|   |--- Speed >  83.00\n",
      "|   |   |--- SpDef <= 71.00\n",
      "|   |   |   |--- class: False\n",
      "|   |   |--- SpDef >  71.00\n",
      "|   |   |   |--- class: True\n",
      "\n",
      "Accuracy (our DT): 0.9187\n",
      "Accuracy (sklearn DT): 0.9187\n"
     ]
    }
   ],
   "source": [
    "features = [\"HP\", \"Attack\", \"Defense\", \"SpAtk\", \"SpDef\", \"Speed\"]\n",
    "X = df[features]\n",
    "y = df[\"Legendary\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Predict using our decision tree\n",
    "y_pred_ours = dt_classify(X_train, y_train, X_test, features, max_depth=3)\n",
    "\n",
    "# Compare with sklearn's DecisionTreeClassifier\n",
    "dt_clf = DecisionTreeClassifier(max_depth=3, criterion=\"entropy\")\n",
    "dt_clf.fit(X_train.values, y_train)\n",
    "\n",
    "# Build and print our decision tree\n",
    "tree = fit_tree(X_train, y_train, features, max_depth=3)\n",
    "print(\"Our Decision Tree:\")\n",
    "print_tree(tree, decimals=2)\n",
    "\n",
    "# Print the tree using sklearn's text exporter for comparison\n",
    "print(\"\\nSklearn Decision Tree:\")\n",
    "print(export_text(dt_clf, feature_names=features, decimals=2, show_weights=False))\n",
    "\n",
    "# Evaluate both\n",
    "print(f\"Accuracy (our DT): {accuracy_score(y_test, y_pred_ours):.4f}\")\n",
    "print(f\"Accuracy (sklearn DT): {accuracy_score(y_test, dt_clf.predict(X_test.values)):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0234234e",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier by \"hand\" (Report)\n",
    "\n",
    "## What is a decision tree\n",
    "A DT is a model that tries to make logic decisions based on question like:\n",
    "\n",
    "- \"The value of HP is lower or equal to 50 (**threshold**)?\"\n",
    "  - If yes, goes to the left\n",
    "  - If not, goes to the right\n",
    "\n",
    "Each **node** of the tree asks a question about a **feature**, and at the end (**on the leafs**), are the **final class** (ex.: ```\"Legendary = True\"``` or ```\"False\"```).\n",
    "\n",
    "## Calculate Entropy\n",
    "\n",
    "```calculate_entropy(y)```\n",
    "\n",
    "This calculates entropy from the given y vector, that is, the mixture between classes.\n",
    "\n",
    "- If there's only 1 class, ```entropy=0``` (Pure).\n",
    "- If there's like a 50/50 between 2 classes, the ```entropy=1``` (maximum uncertainly).\n",
    "\n",
    "``` vals, counts = np.unique(y, return_counts=True)``` - Founds the **unique classes** (ex.: ```[True, False]```) and how many times each one shows.\n",
    "\n",
    "For example:\n",
    "\n",
    "``` python\n",
    "y = [True, False, True, False]\n",
    "vals = [True, False]\n",
    "counts = [0.5, 0.5]\n",
    "```\n",
    "\n",
    "```probs = counts / counts.sum()``` - Convert the counts into probabilities, on this examples the probs are ```[0.5, 0.5]```.\n",
    "\n",
    "Then we return the entropy by using the function entropy from the scipy.stats: ```return stats.entropy(probs)```.\n",
    "This function calculates how mixed the labels are based on the probabilities we just found.\n",
    "\n",
    "## Finding the best threshold for a feature\n",
    "```best_threshold_for_feature(X, y, feature)```\n",
    "\n",
    "Finds the best cut-off point (**threshold**) in a certain **feature**, to divide the data and make classes more \"pure\".\n",
    "\n",
    "For example, taking the feature ```HP```, that assigning imaginary values has ```[10, 20, 30, 40, 50]```, and the classes are, respectively, ```[False, False, True, True, True]```.\n",
    "\n",
    "The code will attempt to cut between unique values, such as:\n",
    "\n",
    "- Between 10 and 20 --> ```thr = 15```\n",
    "- Between 20 and 30 --> ```thr = 25```\n",
    "- Between 30 and 40 --> ```thr = 35```\n",
    "- Between 40 and 50 --> ```thr = 45```\n",
    "\n",
    "At each ```thr```(threshold), divide the data:\n",
    "- Left: values ```<= thr```\n",
    "- Right: values ```> thr```\n",
    "\n",
    "Then it measures the **entropy before and after** the cut and calculates how much it has improved (**information gain**).\n",
    "\n",
    "*In the code:*\n",
    "\n",
    "``` python\n",
    "x = X[feature].to_numpy()\n",
    "y = np.asarray(y)\n",
    "```\n",
    "\n",
    "Convert the data into numpy arrays (to make comparison faster).\n",
    "\n",
    "``` python\n",
    "uniq = np.unique(x)\n",
    "if uniq.size <= 1\n",
    "    return None, 0.0\n",
    "```\n",
    "\n",
    "Removes repeated values from the feature.\n",
    "If after that there‚Äôs only **one unique value**, it means all examples have the same number for this feature (for example, all ```HP = 50```).\n",
    "In that case, there‚Äôs no possible threshold to split.\n",
    "\n",
    "``` python\n",
    "midpoints = (uniq[:-1] + uniq[1:]) / 2.0\n",
    "```\n",
    "\n",
    "Generate the **midpoints** between consecutive values, for example: ```[10, 20, 30] --> (10+20)/2=15, (20+30)/2=25 --> [15, 25]```\n",
    "\n",
    "``` python\n",
    "base_e = calculate_entropy(y)\n",
    "```\n",
    "\n",
    "Calculate entropy before dividing.\n",
    "\n",
    "```\n",
    "for thr in midpoints:\n",
    "    left_mask = x <= thr\n",
    "    right_mask = ~left_mask\n",
    "```\n",
    "Creates boolean masks:\n",
    "\n",
    "- ```left_mask``` --> array of ```True/False``` indicating each one goes to the left.\n",
    "- ```right_mask``` --> the inverse of the left mask\n",
    "\n",
    "``` python\n",
    "yl = y[left_mask]\n",
    "yr = y[right_mask]\n",
    "```\n",
    "We take the classes from the left and the right.\n",
    "\n",
    "``` python\n",
    "el = calculate_entropy_y(yl)\n",
    "er = calculate_entropy_y(yr)\n",
    "```\n",
    "And we calculate the entropy of each side.\n",
    "\n",
    "``` python\n",
    "weighted = (yl.size / n) * el + (yr.size / n) * er\n",
    "gain = base_e - weighted\n",
    "```\n",
    "\n",
    "So that we can calculate the **information gain** (how much the total mixture was reduced).\n",
    "\n",
    "``` python\n",
    "if gain > best_gain:\n",
    "    best_gain = gain\n",
    "    best_thr = thr\n",
    "```\n",
    "\n",
    "We keep the **threshold with the best** **information gain**, and we return both (```return best_thr, best_gain```).\n",
    "\n",
    "## Majority class\n",
    "```majority_class(y)```\n",
    "\n",
    "Chooses the **most frequent class** (in case of a draw, choose the first one).\n",
    "\n",
    "For example: ```[True, False, False, False] --> False```. We use this when the **node** is a **leaf**.\n",
    "\n",
    "## Building the Tree\n",
    "```build_tree(X, y, features, max_depth=None, depth=0)```\n",
    "\n",
    "Creates the tree **recursively**:\n",
    "\n",
    "1. Checks if it is a leaf.\n",
    "2. If not, chooses the best feature + threshold.\n",
    "3. Divides data and repeats.\n",
    "\n",
    "*In the code:*\n",
    "\n",
    "``` python\n",
    "if np.unique(y).size == 1:\n",
    "    return {\"leaf\": True, \"class\": y[0]}\n",
    "```\n",
    "If all classes are equal --> pure node --> **leaf**.\n",
    "\n",
    "``` python\n",
    "if (max_depth is not None and depth >= max_depth):\n",
    "    return {\"leaf\": True, \"class\": majority_class_y(y)}\n",
    "```\n",
    "If maximum depth has been reached --> **Stops here**.\n",
    "\n",
    "``` python\n",
    "best_feat = None\n",
    "best_gain = -1.0\n",
    "best_thr = None\n",
    "```\n",
    "Initialize the variables. ```best_gain``` starts at ```-1.0``` to ensure that any positive gain replaces it.\n",
    "\n",
    "``` python\n",
    "for feature in features:\n",
    "    thr, gain = best_threshold_for_feature(X, y, feature)\n",
    "    if gain > best_gain:\n",
    "        best_gain = gain\n",
    "        best_feat = feature\n",
    "        best_thr = thr\n",
    "```\n",
    "Test **all the features**, see which one has the **best gain**, and keep the best one.\n",
    "\n",
    "``` python\n",
    "if best_thr is None or best_gain <= 0.0:\n",
    "    return {\"leaf\": True, \"class\": majority_class(y)}\n",
    "```\n",
    "If there has been no improvement --> useless node --> make a leaf with the majority class.\n",
    "\n",
    "``` python\n",
    "left_mask = X[best_feat].to_numpy() <= best_thr\n",
    "right_mask = ~left_mask\n",
    "if not left_mask.any() or not right_mask.any():\n",
    "    return {\"leaf\": True, \"class\": majority_class(y)}\n",
    "```\n",
    "Split the data between left and right based on the best feature and threshold.\n",
    "If one side is empty --> useless node --> make a leaf with the majority class.\n",
    "\n",
    "``` python\n",
    "left_child = build_tree(X[left_mask], y[left_mask], features, max_depth, depth + 1)\n",
    "right_child = build_tree(X[right_mask], y[right_mask], features, max_depth, depth + 1)\n",
    "```\n",
    "\n",
    "Call the function again (**recursion**) to build both sides of the tree.\n",
    "\n",
    "``` python\n",
    "return {\n",
    "        \"leaf\": False,\n",
    "        \"feature\": best_feat,\n",
    "        \"threshold\": float(best_thr),\n",
    "        \"left\": left_child,\n",
    "        \"right\": right_child\n",
    "    }\n",
    "```\n",
    "Returns the complete node.\n",
    "\n",
    "## Predict One Sample\n",
    "```predict_one(tree, row)```\n",
    "\n",
    "Traverse a **row** of the tree until reach the final class.\n",
    "\n",
    "For example:\n",
    "\n",
    "``` python\n",
    "[HP <= 80]\n",
    "  -> if 70 <= 80 -> goes left\n",
    "```\n",
    "\n",
    "*In the code:*\n",
    "\n",
    "``` python\n",
    "while not node[\"leaf\"]:\n",
    "    feature = node[\"feature\"]\n",
    "    thr = node[\"threshold\"]\n",
    "    node = node[\"left\"] if row[feature] <= thr else node[\"right\"]\n",
    "return node[\"class\"]\n",
    "```\n",
    "\n",
    "## Predict Tree\n",
    "```predict_tree(tree, X)```\n",
    "\n",
    "Predicts all the rows in ```X``` using the tree.\n",
    "``` python\n",
    "return np.array([predict_one(tree, X.iloc[i]) for i in range(X.shape[0])])\n",
    "``` \n",
    "Uses a list comprehension to call ```predict_one``` for each row. ```iloc``` is used to access rows by their integer index, and ```X.shape[0]``` gives the number of rows in the DataFrame.\n",
    "\n",
    "## Fit Tree\n",
    "```fit_tree(X_train, y_train, features, max_depth=None)```\n",
    "\n",
    "Trains the tree using the training data.\n",
    "\n",
    "``` python\n",
    "return build_tree(X_train, y_train, features, max_depth=max_depth)\n",
    "```\n",
    "\n",
    "## Decision Tree Classify\n",
    "```dt_classify(X_train, y_train, X_test, features, max_depth=None)```\n",
    "\n",
    "Combines everything:\n",
    "1. Fit the tree (```fit_tree```)\n",
    "2. Predict on the test set (```predict_tree```)\n",
    "3. Return the results (```array``` with True/False)\n",
    "\n",
    "``` python\n",
    "tree = fit_tree(X_train, y_train, features, max_depth=max_depth)\n",
    "return predict_tree(tree, X_test)\n",
    "```\n",
    "\n",
    "## Print Tree\n",
    "```print_tree(tree, indent=\"\", decimals=2```\n",
    "\n",
    "Prints the tree in a human-readable format. For example:\n",
    "\n",
    "``` python\n",
    "[HP <= 78.50]\n",
    "  [Speed <= 114.50]\n",
    "    -> class: False\n",
    "  [Speed >  114.50]\n",
    "    -> class: True\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
